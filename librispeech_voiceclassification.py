# -*- coding: utf-8 -*-
"""LibriSpeech_VoiceClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NyxMt3wi_M7xRa89iEmRC496JEmek8Dw
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torchaudio
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader, Subset
import matplotlib.pyplot as plt
from torchsummary import summary

class LibriSpeechDataset(Dataset):
    def __init__(self, root_dir, sample_rate=16000, transform=None, num_sample=16000, device='cpu'):
        self.root_dir = root_dir
        self.sample_rate = sample_rate
        self.transform = transform.to(device) if transform is not None else None
        self.audio_paths = []
        self.transcriptions = []
        self.speaker_ids = []
        self.speaker_map = {}
        self.reverse_speaker_map = {}
        self.num_sample = num_sample
        self.device = device

        current_index = 0

        for root, _, files in os.walk(self.root_dir):
            trans_file = [f for f in files if f.endswith(".trans.txt")]
            if trans_file:
                trans_path = os.path.join(root, trans_file[0])
                speaker_id = os.path.basename(root)

                if speaker_id not in self.speaker_map:
                    self.speaker_map[speaker_id] = current_index
                    self.reverse_speaker_map[current_index] = speaker_id
                    current_index += 1

                speaker_index = self.speaker_map[speaker_id]

                with open(trans_path, "r") as f:
                    lines = f.readlines()
                    for line in lines:
                        parts = line.strip().split(" ", 1)
                        if len(parts) == 2:
                            audio_id, transcription = parts
                            audio_path = os.path.join(root, f"{audio_id}.flac")
                            if os.path.exists(audio_path):
                                self.audio_paths.append(audio_path)
                                self.transcriptions.append(transcription)
                                self.speaker_ids.append(speaker_index)

    def __len__(self):
        return len(self.speaker_ids)

    def __getitem__(self, idx):
        audio_path = self.audio_paths[idx]
        transcription = self.transcriptions[idx]
        speaker_index = self.speaker_ids[idx]
        signal, sr = torchaudio.load(audio_path)
        signal = signal.to(self.device)
        signal = self.__resample__(signal, sr)
        signal = self.__downsample__(signal)
        signal = self.__clamp__(signal)
        if self.transform is not None:
            signal = self.transform(signal)
        return signal, speaker_index

    def get_speaker_id(self, speaker_index):
        return self.reverse_speaker_map[speaker_index]

    def get_num_classes(self):
        return len(self.speaker_map)

    def __resample__(self, signal, sr):
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
            signal = resampler(signal)
        return signal

    def __downsample__(self, signal):
        if signal.shape[0] > 1:
            signal = torch.mean(signal, dim=0, keepdim=True)
        return signal

    def __clamp__(self, signal):
        if signal.shape[1] > self.num_sample:
            signal = signal[:, :self.num_sample]
        if signal.shape[1] < self.num_sample:
            missing_sample = self.num_sample - signal.shape[1]
            pad_dim = (0, missing_sample)
            signal = torch.nn.functional.pad(signal, pad_dim)
        return signal

ROOT_PATH = "drive/MyDrive/LibriSpeech/train-clean-100"
SAMPLE_RATE = 25000
NUM_SAMPLE =  25000
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(DEVICE)
mel_spectrogram = torchaudio.transforms.MelSpectrogram(
    sample_rate = SAMPLE_RATE,
    n_fft=1024,
    hop_length=512,
    n_mels=64
)
libri_dataset = LibriSpeechDataset(ROOT_PATH, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLE)

print(len(libri_dataset))

signal, label = libri_dataset[100]
print(signal.shape)

# Fetch an item from the dataset
signal, label = libri_dataset[18]

# If signal is on a different device, move it to CPU for visualization
signal = signal.cpu()
print(signal)

# Plot the Mel spectrogram
plt.figure(figsize=(10, 6))
plt.imshow(signal[0].detach().numpy(), cmap='inferno', aspect='auto', origin='lower', interpolation='none')
plt.colorbar(format="%+2.0f dB")
plt.title('Mel Spectrogram')
plt.xlabel('Time')
plt.ylabel('Frequency Bins (Mel scale)')
plt.show()

class ClassifierCNN(torch.nn.Module):
  def __init__(self, input_channel, output_channel):
    super().__init__()

    self.conv1 = torch.nn.Sequential(
        torch.nn.Conv2d(input_channel, 16, 3, 1, 2),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(2)
    )
    self.conv2 = torch.nn.Sequential(
        torch.nn.Conv2d(16, 32, 3, 1, 2),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(2)
    )
    self.conv3 = torch.nn.Sequential(
        torch.nn.Conv2d(32, 64, 3, 1, 2),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(2)
    )
    self.conv4 = torch.nn.Sequential(
        torch.nn.Conv2d(64, 128, 3, 1, 2),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(2)
    )
    self.flatten = torch.nn.Flatten()
    self.linear = torch.nn.Linear(128 * 5 * 4, output_channel)
    self.softmax = torch.nn.Softmax(dim=1)

  def forward(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    x = self.flatten(x)
    logits = self.linear(x)
    return logits

cnn = ClassifierCNN(1, 10).to(DEVICE)
summary(cnn, (1,64,49))

print(f"Actual Dataset Size : {len(libri_dataset)}")

DATASET_SIZE = 2000
TEST_RATIO = 0.32

test_size = int(DATASET_SIZE * TEST_RATIO)
train_size = DATASET_SIZE - test_size

subset_indices = np.random.choice(len(libri_dataset), size=DATASET_SIZE, replace=False)

train_indices = subset_indices[:train_size]
test_indices = subset_indices[train_size:]

train_dataset = Subset(libri_dataset, train_indices)
test_dataset = Subset(libri_dataset, test_indices)

print(f"Train Dataset Size: {len(train_dataset)}")
print(f"Test Dataset Size: {len(test_dataset)}")

BATCH_SIZE = 64
train_loader = DataLoader(train_dataset, BATCH_SIZE)
EPOCHS = 10
LEARNING_RATE = 0.001
NUM_CLASSES = libri_dataset.get_num_classes()
model = ClassifierCNN(1, NUM_CLASSES).to(DEVICE)
loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

losses = []
for epoch in range(EPOCHS):
  for i, (input, target) in enumerate(train_loader):
    input, target = input.to(DEVICE), target.to(DEVICE)
    prediction = model(input)
    loss = loss_fn(prediction, target)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    losses.append(loss.item())

    if(i%10 == 0):
      print(f"Loss after epoch {epoch} and iteration {i} : {loss.item():.5f}")

plt.plot(losses)

diff = 0
total = 0
test_loader = DataLoader(test_dataset, 32)
for i, (input, target) in enumerate(train_loader):
    input, target = input.to(DEVICE), target.to(DEVICE)
    prediction = model(input)
    diff += torch.count_nonzero(target - torch.argmax(prediction, dim=1))
    total += torch.numel(target)

print(f"Accuracy = {(1 - diff/total)*100}%")